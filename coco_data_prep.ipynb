{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/\n",
    "#https://github.com/ismailuddin/gradcam-tensorflow-2/blob/master/notebooks/GradCam.ipynb\n",
    "#https://gist.github.com/RaphaelMeudec/e9a805fa82880876f8d89766f0690b54\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from torchvision.models import vgg19, resnet50\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gc\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sriram/.local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:310: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Scale(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) # Use the ImageNet data statistics for normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset class to pre-process the images and return the images and labels to feed into the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetProcessing(Dataset):\n",
    "    def __init__(self, data_path, img_path, ann_path, transform=None, dataType = 'train'):\n",
    "        \n",
    "        self.ann_path = os.path.join(data_path, \"annotations\", ann_path)\n",
    "        self.img_path = os.path.join(data_path, img_path)       \n",
    "\n",
    "        coco=COCO(self.ann_path)\n",
    "        catIDs = coco.getCatIds()\n",
    "        cats = coco.loadCats(catIDs)\n",
    "\n",
    "        annIDs = coco.getAnnIds(catIds=catIDs, iscrowd = False)\n",
    "        anns = coco.loadAnns(ids=annIDs)        \n",
    "\n",
    "        imgIDs = []\n",
    "        labelIDs = []\n",
    "        for i in range(0,len(anns)):\n",
    "            imgIDs.append(anns[i]['image_id'])\n",
    "            labelIDs.append(anns[i]['category_id'])\n",
    "\n",
    "        imgIDs = np.array(imgIDs)\n",
    "        labelIDs = np.array(labelIDs)\n",
    "        uniqueImgIDs = np.unique(imgIDs)\n",
    "\n",
    "        uniqueTargetLabels = []\n",
    "        orderedLabels = np.arange(0,len(catIDs)).astype(np.uint8)\n",
    "        for i in tqdm(range(0,len(uniqueImgIDs))):\n",
    "            targetLabels = labelIDs[imgIDs == uniqueImgIDs[i]]\n",
    "            uniqueLabels = np.unique(targetLabels)\n",
    "            actualLabels = np.zeros(len(uniqueLabels))\n",
    "            for j in range(0,len(uniqueLabels)):\n",
    "                actualLabels[j] = orderedLabels[catIDs == uniqueLabels[j]]\n",
    "            uniqueTargetLabels.append(actualLabels)\n",
    "\n",
    "        uniqueImgIDs = uniqueImgIDs.tolist()\n",
    "        imgs = coco.loadImgs(ids=uniqueImgIDs)\n",
    "\n",
    "        imgFilenames = []\n",
    "        labels = np.zeros((len(imgs),len(catIDs)))\n",
    "        for i in range(0,len(imgs)):\n",
    "            imgFilenames.append(imgs[i]['file_name'])    \n",
    "            uniqueTargetLabels[i] = uniqueTargetLabels[i].astype(np.uint8)        \n",
    "            labels[i,uniqueTargetLabels[i]] = 1\n",
    "\n",
    "        if dataType != 'test':\n",
    "            train_x, val_x, train_y, val_y = train_test_split(imgFilenames, labels, test_size=0.33, random_state=2021)\n",
    "            if dataType == 'train':\n",
    "                self.img_filename = train_x\n",
    "                self.label = train_y\n",
    "            elif dataType == 'val':\n",
    "                self.img_filename = val_x\n",
    "                self.label = val_y\n",
    "        else:\n",
    "            self.img_filename = imgFilenames\n",
    "            self.label = labels\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.img_path, self.img_filename[index]))\n",
    "        img = img.convert('RGB')\n",
    "        img = np.array(img)\n",
    "\n",
    "        def resize_with_padding(im, desired_size = 224):\n",
    "            \"\"\" Resize Image into a square based on desired_size with padding\n",
    "            \"\"\"\n",
    "            old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "\n",
    "            ratio = float(desired_size)/max(old_size)\n",
    "            new_size = tuple([int(x*ratio) for x in old_size])\n",
    "\n",
    "            # new_size should be in (width, height) format\n",
    "\n",
    "            im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "\n",
    "            delta_w = desired_size - new_size[1]\n",
    "            delta_h = desired_size - new_size[0]\n",
    "            top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "            left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "\n",
    "            color = [0, 0, 0]\n",
    "            new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "                value=color)\n",
    "            return new_im\n",
    "\n",
    "        img = resize_with_padding(img)\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = torch.from_numpy(self.label[index])\n",
    "                \n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/media/HD1/sriram/data/coco\"\n",
    "\n",
    "TRAIN_IMG_PATH = \"train2017\"\n",
    "TRAIN_ANN_PATH = \"instances_train2017.json\"\n",
    "\n",
    "TEST_IMG_PATH = \"val2017\"\n",
    "TEST_ANN_PATH = \"instances_val2017.json\"\n",
    "\n",
    "MODEL_PATH_FULL = \"/media/HD1/sriram/models/vgg_train_coco_80classes_v3.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=8.94s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117266/117266 [00:21<00:00, 5483.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=8.47s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117266/117266 [00:21<00:00, 5401.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the training set and validation set\n",
    "train_set = DatasetProcessing(DATA_PATH, TRAIN_IMG_PATH, TRAIN_ANN_PATH, transform = transformations, dataType = 'train')\n",
    "val_set = DatasetProcessing(DATA_PATH, TRAIN_IMG_PATH, TRAIN_ANN_PATH, transform = transformations, dataType = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data loaders for training set and validation set\n",
    "batch_size = 64\n",
    "batch_size_train = 64\n",
    "batch_size_val = 32\n",
    "train_loader = DataLoader(train_set, batch_size = batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size = batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight True\n",
      "features.0.bias True\n",
      "features.2.weight True\n",
      "features.2.bias True\n",
      "features.5.weight True\n",
      "features.5.bias True\n",
      "features.7.weight True\n",
      "features.7.bias True\n",
      "features.10.weight True\n",
      "features.10.bias True\n",
      "features.12.weight True\n",
      "features.12.bias True\n",
      "features.14.weight True\n",
      "features.14.bias True\n",
      "features.16.weight True\n",
      "features.16.bias True\n",
      "features.19.weight True\n",
      "features.19.bias True\n",
      "features.21.weight True\n",
      "features.21.bias True\n",
      "features.23.weight True\n",
      "features.23.bias True\n",
      "features.25.weight True\n",
      "features.25.bias True\n",
      "features.28.weight True\n",
      "features.28.bias True\n",
      "features.30.weight True\n",
      "features.30.bias True\n",
      "features.32.weight True\n",
      "features.32.bias True\n",
      "features.34.weight True\n",
      "features.34.bias True\n",
      "classifier.0.weight True\n",
      "classifier.0.bias True\n",
      "classifier.3.weight True\n",
      "classifier.3.bias True\n",
      "classifier.6.weight True\n",
      "classifier.6.bias True\n"
     ]
    }
   ],
   "source": [
    "# Modify the VGG19 base model to fit the COCO data via transfer learning.\n",
    "num_classes = 80\n",
    "model = vgg19(pretrained = True)\n",
    "model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "# Transfer learning of VGG16 for MSCOCO dataset with 80 classes.\n",
    "# Train only the last FC layer. Freeze the parameters in the other layers.\n",
    "# for param in model.named_parameters():\n",
    "#     if 'classifier' not in param[0]:\n",
    "#         param[1].requires_grad = False\n",
    "\n",
    "for param in model.named_parameters():\n",
    "    print(param[0],param[1].requires_grad)\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 80\n",
    "LOAD_MODEL_PATH = \"/media/HD1/sriram/models/vgg_train_coco_80classes_v2.pt\"\n",
    "model = torch.load(LOAD_MODEL_PATH)\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer and loss function\n",
    "import torch.optim as optim\n",
    "\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    [\n",
    "    {\"params\": model.features[0:10].parameters(), \"lr\": 10e-6},\n",
    "    {\"params\": model.features[10:19].parameters(), \"lr\": 10e-5},\n",
    "    {\"params\": model.features[19:28].parameters(), \"lr\": 10e-4},\n",
    "    {\"params\": model.features[28:].parameters(), \"lr\": 10e-4},\n",
    "    {\"params\": model.classifier.parameters(), \"lr\": 10e-4}\n",
    "    ],\n",
    "    lr = 10e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sriram/.local/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.0006655308 \t Training Accuracy: 66.98 \tValidation Loss: 0.0602281535 \tValidation Accuracy 62.33\n",
      "Validation loss decreased (inf --> 0.0602281535).\n",
      "Epoch: 2 \tTraining Loss: 0.0006644167 \t Training Accuracy: 67.12 \tValidation Loss: 0.0602696852 \tValidation Accuracy 62.52\n",
      "Epoch: 3 \tTraining Loss: 0.0006629610 \t Training Accuracy: 67.22 \tValidation Loss: 0.0602887132 \tValidation Accuracy 62.54\n",
      "Epoch: 4 \tTraining Loss: 0.0006621700 \t Training Accuracy: 67.35 \tValidation Loss: 0.0603170771 \tValidation Accuracy 62.60\n",
      "Epoch: 5 \tTraining Loss: 0.0006620807 \t Training Accuracy: 67.37 \tValidation Loss: 0.0603169101 \tValidation Accuracy 62.58\n",
      "Epoch: 6 \tTraining Loss: 0.0006622182 \t Training Accuracy: 67.34 \tValidation Loss: 0.0603293075 \tValidation Accuracy 62.62\n",
      "Epoch: 7 \tTraining Loss: 0.0006614225 \t Training Accuracy: 67.46 \tValidation Loss: 0.0603470583 \tValidation Accuracy 62.68\n",
      "Epoch: 8 \tTraining Loss: 0.0006613257 \t Training Accuracy: 67.57 \tValidation Loss: 0.0603488956 \tValidation Accuracy 62.64\n",
      "Epoch: 9 \tTraining Loss: 0.0006601651 \t Training Accuracy: 67.53 \tValidation Loss: 0.0603669284 \tValidation Accuracy 62.68\n",
      "Epoch: 10 \tTraining Loss: 0.0006590480 \t Training Accuracy: 67.54 \tValidation Loss: 0.0603855854 \tValidation Accuracy 62.71\n",
      "Epoch: 11 \tTraining Loss: 0.0006596643 \t Training Accuracy: 67.56 \tValidation Loss: 0.0603813229 \tValidation Accuracy 62.69\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 1000\n",
    "early_stop = 0\n",
    "EARLY_STOP_COUNT = 10\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    running_correct_preds = 0\n",
    "    running_labels = 0\n",
    "    model.train()\n",
    "    # model by default is set to train\n",
    "    for batch_i, (data, target) in enumerate(train_loader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "\n",
    "        dummy = torch.zeros((target.shape[0],num_classes)).type(torch.int64)\n",
    "        predictions = torch.sigmoid(output).detach().cpu()\n",
    "        predictions[predictions >= 0.5] = 1\n",
    "        predictions[predictions < 0.5] = 0\n",
    "        correctPreds = torch.where(((predictions > dummy) & (predictions == target.detach().cpu())),1,0)\n",
    "        running_correct_preds += torch.sum(correctPreds)        \n",
    "        running_labels += torch.sum(target)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_accuracy = (running_correct_preds / running_labels) * 100\n",
    "\n",
    "    # Validate the Model\n",
    "    model.eval()\n",
    "    running_correct_preds = 0\n",
    "    running_labels = 0\n",
    "    for data, target in val_loader:\n",
    "        # Move tensor to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # Calculate accuracy on val set\n",
    "        dummy = torch.zeros((target.shape[0],num_classes)).type(torch.int64)\n",
    "        predictions = torch.sigmoid(output).detach().cpu()\n",
    "        predictions[predictions >= 0.5] = 1\n",
    "        predictions[predictions < 0.5] = 0\n",
    "        correctPreds = torch.where(((predictions > dummy) & (predictions == target.detach().cpu())),1,0)\n",
    "        running_correct_preds += torch.sum(correctPreds)        \n",
    "        running_labels += torch.sum(target)\n",
    "\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "\n",
    "    # calculate validation accuracy\n",
    "    val_accuracy = (running_correct_preds/running_labels) * 100\n",
    "\n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.10f} \\t Training Accuracy: {:.2f} \\tValidation Loss: {:.10f} \\tValidation Accuracy {:.2f}'.format(\n",
    "        epoch, train_loss, train_accuracy, valid_loss, val_accuracy))\n",
    "    \n",
    "    # save training/validation statistics\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(valid_loss)    \n",
    "\n",
    "    # Save model at every 100 epochs\n",
    "    if np.mod(epoch,100) == 0:\n",
    "        torch.save(model.save_dict(),MODEL_PATH_FULL)    \n",
    "        print('Saving model')\n",
    "        \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss < valid_loss_min:\n",
    "        print('Validation loss decreased ({:.10f} --> {:.10f}).'.format(valid_loss_min, valid_loss))\n",
    "        valid_loss_min = valid_loss\n",
    "        early_stop = 0\n",
    "    else:\n",
    "        early_stop += 1\n",
    "    if early_stop >= EARLY_STOP_COUNT:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss vs validation loss\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss_list,label=\"Training Loss\")\n",
    "plt.plot(val_loss_list,label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=80, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model, MODEL_PATH_FULL)\n",
    "model = torch.load(MODEL_PATH_FULL)\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the classification accuracy of the trained model on the Val 2017 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.27s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4952/4952 [00:00<00:00, 39880.86it/s]\n"
     ]
    }
   ],
   "source": [
    "test_set = DatasetProcessing(DATA_PATH, TEST_IMG_PATH, TEST_ANN_PATH, transform = transformations, dataType = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4952/4952 [00:32<00:00, 152.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on val 2017 images: 61 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(MODEL_PATH_FULL)\n",
    "model.eval()\n",
    "\n",
    "num_classes = 80\n",
    "running_correct_preds = 0\n",
    "running_labels = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0,len(test_set))):\n",
    "        images, lbls = test_set[i]\n",
    "\n",
    "        img = torch.clone(images)\n",
    "        lbl = torch.clone(lbls)\n",
    "        img = img.unsqueeze(0)\n",
    "        lbl = lbl.unsqueeze(0)\n",
    "        img, lbl = img.cuda(), lbl.cuda()\n",
    "\n",
    "        outputs = model(img)        \n",
    "\n",
    "        dummy = torch.zeros((lbl.shape[0],num_classes)).type(torch.int64)\n",
    "        predictions = torch.sigmoid(outputs).cpu()\n",
    "        predictions[predictions >= 0.5] = 1\n",
    "        predictions[predictions < 0.5] = 0\n",
    "        correctPreds = torch.where(((predictions > dummy) & (predictions == lbl.detach().cpu())),1,0)\n",
    "        running_correct_preds += torch.sum(correctPreds)        \n",
    "        running_labels += torch.sum(lbl)\n",
    "\n",
    "print('Accuracy of the model on val 2017 images: %d %%' % (running_correct_preds / running_labels * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
